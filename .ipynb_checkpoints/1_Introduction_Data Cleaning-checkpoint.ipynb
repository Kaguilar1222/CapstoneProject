{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "One of the most fascinating debates in literature is the authorship of Shakespeare's plays. My objective with this project is to explore various NLP methods and to use Elizabethan-era plays for a supervised classification task to determine if a play is or is not written by Shakespeare. I have 39 .txt files for plays that were written by Shakespeare (two of which are only partial, more recent attributions) and 50 .txt files that are attributed to Shakespeare's contemporaries.\n",
    "\n",
    "As I am only dealing with a slight class imbalance, I am not making any changes to the data to correct for this when building my authorship attribution models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:40:48.683210Z",
     "start_time": "2020-06-09T21:40:46.079808Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing libraries for file directories, arrays, tf-idf, and tokenization\n",
    "import os, shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "import random\n",
    "np.random.seed(0)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note: I added Pericles, Two Noble Kinsmen and Edward III to the Shakespeare data as they are generally recognized as partial or full-attributions by Shakespearean scholars and the Folger Shakespeare Library. The main Shakespearean corpus of 37 does not include these 3 plays, and often only the original 37 are recognized.\n",
    "\n",
    "The same is not true of Lucrice, Cromwell, and Sir John Oldcastle (among other plays which Project Gutenberg attributes to Shakespeare) which had various turns being published under Shakespeare's name. Although these are occasionally still debated, it is often thought that publishers intentionally used Shakespeare's name to sell the plays under false pretenses and are generally accepted outside of Project Gutenberg not to be written by him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Train/Test Split\n",
    "\n",
    "Below in comments are the lines of code I used to divide out my train/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:40:48.712501Z",
     "start_time": "2020-06-09T21:40:48.702851Z"
    }
   },
   "outputs": [],
   "source": [
    "# # saving filepaths for original folders\n",
    "# non_shakespeare_directory = 'data/Other'\n",
    "# shakespeare_directory = 'data/Shakespeare'\n",
    "# non_shakespeare_filenames = os.listdir(non_shakespeare_directory)\n",
    "# shakespeare_filenames = os.listdir(shakespeare_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:40:48.744041Z",
     "start_time": "2020-06-09T21:40:48.729312Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving filepaths for new train/test folders \n",
    "train_folder = os.path.join('data/', 'train')\n",
    "train_shakespeare = os.path.join(train_folder, 'shakespeare')\n",
    "train_not_shakespeare = os.path.join(train_folder, 'not_shakespeare')\n",
    "\n",
    "test_folder = os.path.join('data/', 'test')\n",
    "test_shakespeare = os.path.join(test_folder, 'shakespeare')\n",
    "test_not_shakespeare = os.path.join(test_folder, 'not_shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:40:48.792233Z",
     "start_time": "2020-06-09T21:40:48.779700Z"
    }
   },
   "outputs": [],
   "source": [
    "# # implementing train/test split\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_shakespeare)\n",
    "# os.mkdir(test_not_shakespeare)\n",
    "\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_shakespeare)\n",
    "# os.mkdir(train_not_shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:40:48.810020Z",
     "start_time": "2020-06-09T21:40:48.801662Z"
    }
   },
   "outputs": [],
   "source": [
    "# # creating test/train split in folders \n",
    "# import random\n",
    "\n",
    "# # moving 9 random Shakespeare test plays\n",
    "# for play in random.sample(os.listdir(shakespeare_directory), k=9):\n",
    "#     origin = os.path.join(shakespeare_directory, play)\n",
    "#     destination = os.path.join(test_shakespeare, play)\n",
    "#     shutil.move(origin, destination)\n",
    "    \n",
    "# # train shakespeare\n",
    "# for play in os.listdir(shakespeare_directory):\n",
    "#     origin = os.path.join(shakespeare_directory, play)\n",
    "#     destination = os.path.join(train_shakespeare, play)\n",
    "#     shutil.move(origin, destination)\n",
    "    \n",
    "# # moving 9 random non-Shakespeare test plays\n",
    "# for play in random.sample(os.listdir(non_shakespeare_directory), k=9):\n",
    "#     origin = os.path.join(non_shakespeare_directory, play)\n",
    "#     destination = os.path.join(test_not_shakespeare, play)\n",
    "#     shutil.move(origin, destination)\n",
    "    \n",
    "# # train non-shakespeare\n",
    "# for play in os.listdir(non_shakespeare_directory):\n",
    "#     origin = os.path.join(non_shakespeare_directory, play)\n",
    "#     destination = os.path.join(train_not_shakespeare, play)\n",
    "#     shutil.move(origin, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Now that my files for each class are split out into train/test data, I am setting up this data to be cleaned and vectorized. I will be saving my data in 3 different forms for use in other notebooks:\n",
    "* TF-IDF of word vectors\n",
    "* TF-IDF of word vectors with dimensionality reduction\n",
    "* TF-IDF of bigrams and trigrams together\n",
    "* Lemmatized plays (still containing stopwords and punctuation)\n",
    "* Tokenized plays (stopwords removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Notes\n",
    "\n",
    "There are a lot of editors notes in each of the plays - not only introductions from scholars and historians, but also general usage legal language inserted by Project Gutenberg. At this stage I am also removing stopwords and punctuation. The stopwords list is supplemented by an Elizabethan-era stopwords list made available by Bryan Bumgardner here: https://bryanbumgardner.com/elizabethan-stop-words-for-nlp/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:42:07.499923Z",
     "start_time": "2020-06-09T21:42:07.468334Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining stopwords list\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += ['art', 'doth', 'dost', 'ere', 'ere', 'hast', 'hath',\n",
    "                   'hence', 'hither', 'nigh', 'oft', 'shouldst', 'thither',\n",
    "                   'thee', 'thou', 'thine', 'thy', 'tis', 'twas', 'wast',\n",
    "                   'whence', 'wherefore', 'whereto', 'withal', 'wouldst',\n",
    "                   'ye', 'yon', 'yonder', 'th']\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "# defining regex pattern\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:42:06.786719Z",
     "start_time": "2020-06-09T21:42:06.778259Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining function to clean play data and remove editors notes\n",
    "def remove_header(play):\n",
    "    play_lower = [line.lower() for line in play]\n",
    "    removed_notes = []\n",
    "# deleting editors notes at beginning of text \n",
    "    for i, line in enumerate(play_lower):\n",
    "        if ('dramatis person' in line)\\\n",
    "        or ('drammatis person' in line)\\\n",
    "        or ('persons represented' in line)\\\n",
    "        or ('the actors names' in line)\\\n",
    "        or ('Persons of the' in line)\\\n",
    "        or ('printed by iohn norton:' in line):\n",
    "            removed_notes = play_lower[i+1:]\n",
    "            break\n",
    "        else:\n",
    "            removed_notes = play_lower\n",
    "    return removed_notes\n",
    "\n",
    "\n",
    "def remove_appendix(prologue_removed):\n",
    "    removed_appendix = []\n",
    "    for i, line in enumerate(prologue_removed):\n",
    "        if ('end of this project gutenberg ebook' in line):\n",
    "            removed_appendix = prologue_removed[:i]\n",
    "            break\n",
    "        else:\n",
    "            removed_appendix = prologue_removed\n",
    "    return removed_appendix\n",
    "\n",
    "# deleting empty lines \n",
    "def remove_blank_lines(removed_notes):\n",
    "    for i, line in enumerate(removed_notes):\n",
    "        if (line == '\\n'):\n",
    "            del removed_notes[i]\n",
    "    return removed_notes\n",
    "\n",
    "bad_words = ['gutenberg', 'commercial', 'copyright', 'shakespeare',\\\n",
    "             'full license', 'united states', 'carnegie mellon',\\\n",
    "            'donations', 'ebook', 'legal', 'ascii', 'electronic', 'download'\\\n",
    "            'online', 'restrictions', '@']\n",
    "bad_words += [str(i) for i in list(range(10))]\n",
    "\n",
    "      \n",
    "# deleting line breaks from text, replacing archaic 's', removing footnotes where remaining\n",
    "def remove_notes(removed_notes):\n",
    "    edited_play = []\n",
    "    for line in removed_notes:\n",
    "        line = line.replace('Å¿', 's').replace('\\n', '')\\\n",
    "        .replace('\\\\', '')\n",
    "        if not any (word in line for word in bad_words):\n",
    "            edited_play.append(line)\n",
    "    return edited_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:42:09.100469Z",
     "start_time": "2020-06-09T21:42:07.695047Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  and humbly now upon my bended knee,     in sight of england and her lordly peers,     deliver up my title in the queen     to your most gracious hands, that are the substance     of that great shado'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating training/test corpus and target arrays\n",
    "def main(shakes_directory, not_shakes_directory):\n",
    "    corpus = []\n",
    "    list_target = []\n",
    "    # Iterate through list of filenames and read each it\n",
    "    for file in os.listdir(shakes_directory):\n",
    "        with open(shakes_directory + '/' + file, encoding='utf8', errors='ignore') as f:\n",
    "            raw_data = f.readlines()\n",
    "        # cleaning raw text\n",
    "        header_removed = remove_header(raw_data)\n",
    "        appendix_removed = remove_appendix(header_removed)\n",
    "        blanks_removed = remove_blank_lines(appendix_removed)\n",
    "        cleaned = remove_notes(blanks_removed)\n",
    "        corpus.append(' '.join(cleaned))\n",
    "        list_target.append(1)\n",
    "    for file in os.listdir(not_shakes_directory):\n",
    "        with open(not_shakes_directory + '/' + file, encoding='utf8', errors='ignore') as f:\n",
    "            raw_data = f.readlines()\n",
    "        # cleaning raw text\n",
    "        header_removed = remove_header(raw_data)\n",
    "        appendix_removed = remove_appendix(header_removed)\n",
    "        blanks_removed = remove_blank_lines(appendix_removed)\n",
    "        cleaned = remove_notes(blanks_removed)\n",
    "        corpus.append(' '.join(cleaned))\n",
    "        list_target.append(0)\n",
    "    return corpus, np.array(list_target)\n",
    "\n",
    "        \n",
    "train_corpus, y_train = main(train_shakespeare, train_not_shakespeare)\n",
    "test_corpus, y_test = main(test_shakespeare, test_not_shakespeare)\n",
    "\n",
    "train_corpus[0][2000:2200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Lemmatized Plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:42:14.286849Z",
     "start_time": "2020-06-09T21:42:14.088630Z"
    }
   },
   "outputs": [],
   "source": [
    "# preparing plays by converting lines into sentences that can be POS tagged and lemmatized\n",
    "train_corpus_sentences = [re.split('[._!?]', play) for play in train_corpus]\n",
    "test_corpus_sentences = [re.split('[._!?]', play) for play in test_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:42:20.301694Z",
     "start_time": "2020-06-09T21:42:20.290969Z"
    }
   },
   "outputs": [],
   "source": [
    "# initializing lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# getting POS tags from wordnet (we only need the first letter of the tag)\n",
    "def get_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:                    \n",
    "        return None\n",
    "\n",
    "# tokenizing sentence, getting POS tag, creating tuple of word and tag generated above\n",
    "def lemmatize_plays(corpus):\n",
    "    corpus_to_return = []\n",
    "    for play in corpus:\n",
    "        lemmatized_sentences = [] \n",
    "        for sentence in play:\n",
    "            nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))    \n",
    "            wn_tagged = map(lambda x: (x[0], get_pos(x[1])), nltk_tagged)\n",
    "            res_words = []\n",
    "            # returning lemmatized sentence and returning list of sentences\n",
    "            for word, tag in wn_tagged:\n",
    "                if tag is None:                        \n",
    "                    res_words.append(word)\n",
    "                else:\n",
    "                    res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "            lemmatized_sentences.append(' '.join(res_words))\n",
    "        corpus_to_return.append(' '.join(lemmatized_sentences))\n",
    "    return corpus_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:47:39.610900Z",
     "start_time": "2020-06-09T21:42:25.624624Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizing, lemmatizing \n",
    "train_lemmatized = lemmatize_plays(train_corpus_sentences)\n",
    "test_lemmatized = lemmatize_plays(test_corpus_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Play Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:47:43.117785Z",
     "start_time": "2020-06-09T21:47:42.939065Z"
    }
   },
   "outputs": [],
   "source": [
    "# stripping stopwords, tokenizing\n",
    "def tokenize_stop(lemmatized_plays):\n",
    "    tokens = []\n",
    "    for play in lemmatized_plays:\n",
    "        tokens_raw = nltk.regexp_tokenize(play, pattern)\n",
    "        stopped_play = [word for word in tokens_raw if word not in stopwords_list]\n",
    "        tokens.append(stopped_play)\n",
    "    return tokens\n",
    "train_tokens = tokenize_stop(train_lemmatized)\n",
    "test_tokens = tokenize_stop(test_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TF-IDFs\n",
    "\n",
    "Below I am creating TF-IDF vector representations of the texts, and saving both vectors with all words in the corpus, and a dimensionality reduced representation that still has 80% of explained variance (this should hopefully address any overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:47:48.900584Z",
     "start_time": "2020-06-09T21:47:46.763148Z"
    }
   },
   "outputs": [],
   "source": [
    "# initializing TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords_list, token_pattern=pattern)\n",
    "X_train = tfidf.fit_transform(train_lemmatized)\n",
    "X_test = tfidf.transform(test_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:47:52.877149Z",
     "start_time": "2020-06-09T21:47:52.038687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8057528066646362\n"
     ]
    }
   ],
   "source": [
    "# importing library to lower dimensions of TF-IDF vector in a way that keeps 80% of explained variance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# creating lower-dimension TF-IDF vectors \n",
    "svd = TruncatedSVD(n_components=50)\n",
    "X_train_50 = svd.fit_transform(X_train)\n",
    "X_test_50 = svd.transform(X_test)\n",
    "print(np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:47:58.312646Z",
     "start_time": "2020-06-09T21:47:56.264880Z"
    }
   },
   "outputs": [],
   "source": [
    "# initializing TF-IDF vectorizer for bigrams\n",
    "tfidf_ngrams = TfidfVectorizer(stop_words=stopwords_list,\n",
    "                               token_pattern=pattern,\n",
    "                               ngram_range=(2, 2))\n",
    "\n",
    "X_train_ngrams = tfidf_ngrams.fit_transform(train_lemmatized)\n",
    "X_test_ngrams = tfidf_ngrams.transform(test_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to Pickle\n",
    "\n",
    "I am saving all 5 different representations of my text for use in other notebooks; as well, of course, as the true class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T21:48:02.740626Z",
     "start_time": "2020-06-09T21:48:02.676905Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving files as pickle objects for use in other notebooks\n",
    "import pickle\n",
    "\n",
    "pickle_jar = [X_train, X_test, \n",
    "              y_train, y_test,\n",
    "              X_train_50, X_test_50,\n",
    "              X_train_ngrams, X_test_ngrams,\n",
    "              train_lemmatized, test_lemmatized,\n",
    "              train_tokens, test_tokens]\n",
    "\n",
    "with open('data/pickle_jar.pickle', 'wb') as f:\n",
    "    pickle.dump(pickle_jar, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook I will be exploring methods for visualizing our data, to identify some common words, see how separable our classes are, and how we can use vectors to determine word similarities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "182.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
